{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SirigineediDivya/FMML_LABS_PROJECTS_26/blob/main/FMML_2024_Module_5_Lab_1_NL_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjvepVoWOJU_"
      },
      "source": [
        "#Module 5 Lab 1\n",
        "\n",
        "# Non Linear Support Vector Machines\n",
        "\n",
        "```\n",
        "Module Coordinator : Nikunj Nawal\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVMs Recap:\n",
        "\n",
        "![SVM](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1526288453/index3_souoaz.png)\n",
        "\n",
        "SVMs are an iterative approach of trying to find the optimal hyperplane that divides the multidimentional space into different classes present in the dataset.\n",
        "\n",
        "\n",
        "**Hyperplanes:** These are the decision planes that separate the objects of classes that we are trying to classify.\n",
        "\n",
        "**Support Vectors** : Support vectors are the points from dataset that are closest to the hyperplane that divides the dataset.\n",
        "\n",
        "**Margin**:  The gap between the closest support vectors from the different class along the direction perpendicular to the hyperplane. Simply put, it is the sum of perpendicular distance of the support vector of each class to the hyperplane.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "hq4srNYmhMOA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjCAglTcNeGi"
      },
      "source": [
        "# Importing the necessary packages\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn import svm\n",
        "\n",
        "import pandas\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffq0iFPad5dq"
      },
      "source": [
        "The topic of classifier in today's lab, SVMs make for really good linear separators. Let us look at an example which has linearly separable data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wSo9Y7DWTth"
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "ar = np.vstack(     [\\\n",
        "                    np.random.multivariate_normal(np.array([1, 1]), 1.5 * np.array([[2, -1], [-1, 2.0]]), size = 50, ),\\\n",
        "                    np.random.multivariate_normal(np.array([3, 3]), 2 * np.array([[0.75, -0.5], [-0.5, 0.75]]), size = 50, )\n",
        "                    ]\\\n",
        "              )\n",
        "\n",
        "testAr = np.vstack(   [\\\n",
        "                      np.random.multivariate_normal(np.array([1, 1]), np.array([[0.5, -0.25], [-0.25, 0.5]]), size = 500, ),\\\n",
        "                      np.random.multivariate_normal(np.array([3, 3]), np.array([[0.75, -0.5], [-0.5, 0.75]]), size = 500, )\n",
        "                      ]\\\n",
        "                  )\n",
        "testy = np.array([0] * int((testAr.shape[0]/2)) + [1] * int((testAr.shape[0]/2)))\n",
        "\n",
        "X = ar\n",
        "y = np.array([0] * int((ar.shape[0]/2)) + [1] * int((ar.shape[0]/2)))\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "def plotDecisionBoundary(X, y, pair, clf):\n",
        "  x_min, x_max = X[:, pair[0]].min() - 1, X[:, pair[0]].max() + 1\n",
        "  y_min, y_max = X[:, pair[1]].min() - 1, X[:, pair[1]].max() + 1\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                      np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "  y_pred = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "  y_pred = y_pred.reshape(xx.shape)\n",
        "  plt.figure(figsize=(5,4))\n",
        "  plt.contourf(xx, yy, y_pred, alpha=0.4)\n",
        "  plt.scatter(X[:, pair[0]], X[:, pair[1]], c = y, s = 50, edgecolor='k')\n",
        "  plt.legend([\"0\", \"1\"])\n",
        "  plt.gcf().set_dpi(130)\n",
        "  plt.show()\n",
        "\n",
        "def boundaryExp() :\n",
        "  clf = svm.LinearSVC()\n",
        "  pair = [0, 1]\n",
        "  clf.fit(X[:, pair], y)\n",
        "  plotDecisionBoundary(X, y, pair, clf)\n",
        "  plt.show()\n",
        "\n",
        "boundaryExp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtn7FxSZqRj8"
      },
      "source": [
        "# The Kernel Trick\n",
        "\n",
        "The true potential of SVMs is unleashed when they are combined with kernels.\n",
        "\n",
        "## Kernels : An intuitive explanation\n",
        "\n",
        "Kernel methods are essentially counting on using the training data (say $i^{th}$ example $(x_i, y_i)$ ) itself in a more straightforward way and learning a corresponding weight ($w_i$) for that example. Rather than trying to learn a fixed set of parameters which is done typically.\n",
        "Depending on the kind of kernel used, we can virtually project the training data in a higher dimension to make it easier for the classifier to classify them.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpzUIM1ppnxX"
      },
      "source": [
        "from sklearn.datasets import make_circles\n",
        "X, y = make_circles(200, factor=.1, noise=.1)\n",
        "\n",
        "clf = svm.SVC(kernel='linear').fit(X, y)\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8\")\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='tab20')\n",
        "plt.gcf().set_dpi(110)\n",
        "plt.legend([\"0\", \"1\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cInt5fSty3w"
      },
      "source": [
        "plt.style.use(\"default\")\n",
        "clf = svm.LinearSVC()\n",
        "pair = [0, 1]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, [0, 1], clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJaPFAniusEA"
      },
      "source": [
        "However, if we artificially add another dimention to the dataset of the form:\n",
        "\n",
        "$z = x^2 + y^2$\n",
        "we can clearly see a hyperplane that can distinguish both the datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQamaPU8vfIz"
      },
      "source": [
        "Z = np.array([[i[0]**2 + i[1]**2] for i in X])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFi_pqMMvyPS"
      },
      "source": [
        "X_new = np.hstack((X, Z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JuoK_qjwPr_"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "fig = go.Figure(data = [go.Scatter3d(\n",
        "    x = X_new[:, 0],\n",
        "    y = X_new[:, 1],\n",
        "    z = X_new[:, 2],\n",
        "    mode = \"markers\",\n",
        "    marker = {\n",
        "        \"color\" : y,\n",
        "        \"line\": {\"width\" : 4, \"color\":'DarkSlateGrey'},\n",
        "        \"colorscale\": \"viridis\"},\n",
        ")])\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdGH5BF2xuab"
      },
      "source": [
        "That simple trick has helped us to get another dimension in which the data is linearly separable by a hyperplane (in this case, a 2d plane)\n",
        "\n",
        "---\n",
        "\n",
        "Now let us use the rbf kernel and use an SVM Classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybfpMPLFyIiZ"
      },
      "source": [
        "plt.style.use(\"default\")\n",
        "clf = svm.SVC(kernel='rbf')\n",
        "pair = [0, 1]\n",
        "clf.fit(X[:, pair], y)\n",
        "plotDecisionBoundary(X, y, [0, 1], clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHwwqJzHyt23"
      },
      "source": [
        "Now let us get back to our original dataset of iris and see if this kernel trick has helped us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6RS1AGtyfYs"
      },
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
        "clf = svm.SVC(kernel='poly')\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy of the decision tree on the test set: \\n\\n{:.3f}\\n\\n\".format(accuracy_score(y_pred, y_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29evrc-jy0WO"
      },
      "source": [
        "Certainly, using a kernel has increased our accuracy on the iris dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Tasks:\n",
        "\n",
        "Task-1\n",
        "Try to run the same experiment after filtering different features, 2 at a time.\n",
        "\n",
        "Task-2\n",
        "Use different kinds of kernels for the SVM and plot it for Iris dataset -\n",
        "1. linear\n",
        "2. poly\n",
        "3. rbf\n",
        "4. sigmoid\n",
        "5. precomputed\n",
        "\n",
        "Learning Tasks:\n",
        "Learn about SVM types, support vectors and hyperplanes."
      ],
      "metadata": {
        "id": "LteLWVYtYu8m"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XaGYYjZgwToJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}